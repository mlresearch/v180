---
title: Dimension reduction for high-dimensional small counts with KL divergence
abstract: 'Dimension reduction for high-dimensional count data with a large proportion
  of zeros is an important task in various applications. As a large number of dimension
  reduction methods rely on the proximity measure, we develop a dissimilarity measure
  that is well-suited for small counts based on the Kullback-Leibler divergence. We
  compare the proposed measure with other widely used dissimilarity measures and show
  that the proposed one has superior discriminative ability when applied to high-dimensional
  count data having an excess of zeros. Extensive empirical results, on both simulated
  and publicly-available real-world datasets that contain many zeros, demonstrate
  that the proposed dissimilarity measure can improve a wide range of dimension reduction
  methods. '
openreview: BhzEFwLj5l5
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ling22a
month: 0
tex_title: Dimension reduction for high-dimensional small counts with KL divergence
firstpage: 1210
lastpage: 1220
page: 1210-1220
order: 1210
cycles: false
bibtex_author: Ling, Yurong and Xue, Jing-Hao
author:
- given: Yurong
  family: Ling
- given: Jing-Hao
  family: Xue
date: 2022-08-17
address:
container-title: Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial
  Intelligence
volume: '180'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 8
  - 17
pdf: https://proceedings.mlr.press/v180/ling22a/ling22a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v180/ling22a/ling22a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
