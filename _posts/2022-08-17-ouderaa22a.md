---
title: Learning invariant weights in neural networks
abstract: Assumptions about invariances or symmetries in data can significantly increase
  the predictive power of statistical models. Many commonly used machine learning
  models are constraint to respect certain symmetries, such as translation equivariance
  in convolutional neural networks, and incorporating other symmetry types is actively
  being studied. Yet, learning invariances from the data itself remains an open research
  problem. It has been shown that the marginal likelihood offers a principled way
  to learn invariances in Gaussian Processes. We propose a weight-space equivalent
  to this approach, by minimizing a lower bound on the marginal likelihood to learn
  invariances in neural networks, resulting in naturally higher performing models.
openreview: BVxfSPUoqeq
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ouderaa22a
month: 0
tex_title: Learning invariant weights in neural networks
firstpage: 1992
lastpage: 2001
page: 1992-2001
order: 1992
cycles: false
bibtex_author: van der Ouderaa, Tycho F.A. and van der Wilk, Mark
author:
- given: Tycho F.A.
  family: Ouderaa
  prefix: van der
- given: Mark
  family: Wilk
  prefix: van der
date: 2022-08-17
address:
container-title: Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial
  Intelligence
volume: '180'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 8
  - 17
pdf: https://proceedings.mlr.press/v180/ouderaa22a/ouderaa22a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v180/ouderaa22a/ouderaa22a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
