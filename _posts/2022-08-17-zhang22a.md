---
title: Distributed adversarial training to robustify deep neural networks at scale
abstract: Current deep neural networks (DNNs) are vulnerable to adversarial attacks,
  where adversarial perturbations to the inputs can change or manipulate classification.
  To defend against such attacks, an effective and popular approach, known as adversarial
  training (AT), has been shown to mitigate the negative impact of adversarial attacks
  by virtue of a min-max robust training method. While effective, it remains unclear
  whether it can successfully be adapted to the distributed learning context. The
  power of distributed optimization over multiple machines enables us to scale up
  robust training over large models and datasets. Spurred by that, we propose distributed
  adversarial training (DAT), a large-batch adversarial training framework implemented
  over multiple machines. We show that DAT is general, which supports training over
  labeled and unlabeled data, multiple types of attack generation methods, and gradient
  compression operations favored for distributed optimization. Theoretically, we provide,
  under standard conditions in the optimization theory, the convergence rate of DAT
  to the first-order stationary points in general non-convex settings. Empirically,
  we demonstrate that DAT either matches or outperforms state-of-the-art robust accuracies
  and achieves a graceful training speedup (e.g., on ResNet-50 under ImageNet). Codes
  are available at https://github.com/dat-2022/dat.
openreview: Srgg_ULj9gq
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang22a
month: 0
tex_title: Distributed adversarial training to robustify deep neural networks at scale
firstpage: 2353
lastpage: 2363
page: 2353-2363
order: 2353
cycles: false
bibtex_author: Zhang, Gaoyuan and Lu, Songtao and Zhang, Yihua and Chen, Xiangyi and
  Chen, Pin-Yu and Fan, Quanfu and Martie, Lee and Horesh, Lior and Hong, Mingyi and
  Liu, Sijia
author:
- given: Gaoyuan
  family: Zhang
- given: Songtao
  family: Lu
- given: Yihua
  family: Zhang
- given: Xiangyi
  family: Chen
- given: Pin-Yu
  family: Chen
- given: Quanfu
  family: Fan
- given: Lee
  family: Martie
- given: Lior
  family: Horesh
- given: Mingyi
  family: Hong
- given: Sijia
  family: Liu
date: 2022-08-17
address:
container-title: Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial
  Intelligence
volume: '180'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 8
  - 17
pdf: https://proceedings.mlr.press/v180/zhang22a/zhang22a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v180/zhang22a/zhang22a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
