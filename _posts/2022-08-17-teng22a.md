---
title: Contrastive latent variable models for neural text generation
abstract: 'Deep latent variable models such as variational autoencoders and energy-based
  models are widely used for neural text generation. Most of them focus on matching
  the prior distribution with the posterior distribution of the latent variable for
  text reconstruction. In addition to instance-level reconstruction, this paper aims
  to integrate contrastive learning in the latent space, forcing the latent variables
  to learn high-level semantics by exploring inter-instance relationships. Experiments
  on various text generation benchmarks show the effectiveness of our proposed method.
  We also empirically show that our method can mitigate the posterior collapse issue
  for latent variable based text generation models. '
openreview: HMMlduUicg9
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: teng22a
month: 0
tex_title: Contrastive latent variable models for neural text generation
firstpage: 1928
lastpage: 1938
page: 1928-1938
order: 1928
cycles: false
bibtex_author: Teng, Zhiyang and Chen, Chenhua and Zhang, Yan and Zhang, Yue
author:
- given: Zhiyang
  family: Teng
- given: Chenhua
  family: Chen
- given: Yan
  family: Zhang
- given: Yue
  family: Zhang
date: 2022-08-17
address:
container-title: Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial
  Intelligence
volume: '180'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 8
  - 17
pdf: https://proceedings.mlr.press/v180/teng22a/teng22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
