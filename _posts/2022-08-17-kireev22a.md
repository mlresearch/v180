---
title: On the effectiveness of adversarial training against common corruptions
abstract: The literature on robustness towards common corruptions shows no consensus
  on whether adversarial training can improve the performance in this setting. First,
  we show that, when used with an appropriately selected perturbation radius, Lp adversarial
  training can serve as a strong baseline against common corruptions improving both
  accuracy and calibration. Then we explain why adversarial training performs better
  than data augmentation with simple Gaussian noise which has been observed to be
  a meaningful baseline on common corruptions. Related to this, we identify the sigma-overfitting
  phenomenon when Gaussian augmentation overfits to a particular standard deviation
  used for training which has a significant detrimental effect on common corruption
  accuracy. We discuss how to alleviate this problem and then how to further enhance
  Lp adversarial training by introducing an efficient relaxation of adversarial training
  with learned perceptual image patch similarity as the distance metric. Through experiments
  on CIFAR-10 and ImageNet-100, we show that our approach does not only improve the
  Lp adversarial training baseline but also has cumulative gains with data augmentation
  methods such as AugMix, DeepAugment, ANT, and SIN, leading to state-of-the-art performance
  on common corruptions.
openreview: BcU_UIIjqg9
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kireev22a
month: 0
tex_title: On the effectiveness of adversarial training against common corruptions
firstpage: 1012
lastpage: 1021
page: 1012-1021
order: 1012
cycles: false
bibtex_author: Kireev, Klim and Andriushchenko, Maksym and Flammarion, Nicolas
author:
- given: Klim
  family: Kireev
- given: Maksym
  family: Andriushchenko
- given: Nicolas
  family: Flammarion
date: 2022-08-17
address:
container-title: Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial
  Intelligence
volume: '180'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 8
  - 17
pdf: https://proceedings.mlr.press/v180/kireev22a/kireev22a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v180/kireev22a/kireev22a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
