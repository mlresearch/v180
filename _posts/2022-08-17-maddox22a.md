---
title: Low-precision arithmetic for fast Gaussian processes
abstract: Low precision arithmetic has had a transformative effect on the training
  of neural networks, reducing computation, memory and energy requirements. However,
  despite their promise, low precision operations have received little attention for
  Gaussian process (GP) training, largely because GPs require sophisticated linear
  algebra routines that are unstable in low precision. We study the different failure
  modes that can occur when training GPs in half-precision. To circumvent these failure
  modes, we propose a multi-faceted approach involving conjugate gradients with re-orthogonalization,
  mixed precision, compact kernels, and preconditioners. Our approach significantly
  improves the numerical stability and practical performance of conjugate gradients
  in low precision over a wide range of settings, and reduces the runtime of 1.8 million
  data points to 10 hours on a single GPU, without requiring any sparse approximations.
openreview: S3NOX_Ij9xc
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: maddox22a
month: 0
tex_title: Low-precision arithmetic for fast Gaussian processes
firstpage: 1306
lastpage: 1316
page: 1306-1316
order: 1306
cycles: false
bibtex_author: Maddox, Wesley J. and Potapcynski, Andres and Wilson, Andrew Gordon
author:
- given: Wesley J.
  family: Maddox
- given: Andres
  family: Potapcynski
- given: Andrew Gordon
  family: Wilson
date: 2022-08-17
address:
container-title: Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial
  Intelligence
volume: '180'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 8
  - 17
pdf: https://proceedings.mlr.press/v180/maddox22a/maddox22a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v180/maddox22a/maddox22a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
