---
title: 'Stability of SGD: Tightness analysis and improved bounds'
abstract: 'Stochastic Gradient Descent (SGD) based methods have been widely used for
  training large-scale machine learning models that also generalize well in practice.
  Several explanations have been offered for this generalization performance, a prominent
  one being algorithmic stability Hardt et al [2016]. However, there are no known
  examples of smooth loss functions for which the analysis can be shown to be tight.
  Furthermore, apart from properties of the loss function, data distribution has also
  been shown to be an important factor in generalization performance. This raises
  the question: is the stability analysis of Hardt et al [2016] tight for smooth functions,
  and if not, for what kind of loss functions and data distributions can the stability
  analysis be improved? In this paper we first settle open questions regarding tightness
  of bounds in the data-independent setting: we show that for general datasets, the
  existing analysis for convex and strongly-convex loss functions is tight, but it
  can be improved for non-convex loss functions. Next, we give novel and improved
  data-dependent bounds: we show stability upper bounds for a large class of convex
  regularized loss functions, with negligible  regularization parameters, and improve
  existing data-dependent bounds in the non-convex setting. We hope that our results
  will initiate further efforts to better understand the data-dependent setting under
  non-convex loss functions, leading to an improved understanding of the generalization
  abilities of deep networks.'
openreview: Sl-zmO8j5lq
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang22b
month: 0
tex_title: 'Stability of {SGD}: Tightness analysis and improved bounds'
firstpage: 2364
lastpage: 2373
page: 2364-2373
order: 2364
cycles: false
bibtex_author: Zhang, Yikai and Zhang, Wenjia and Bald, Sammy and Pingali, Vamsi and
  Chen, Chao and Goswami, Mayank
author:
- given: Yikai
  family: Zhang
- given: Wenjia
  family: Zhang
- given: Sammy
  family: Bald
- given: Vamsi
  family: Pingali
- given: Chao
  family: Chen
- given: Mayank
  family: Goswami
date: 2022-08-17
address:
container-title: Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial
  Intelligence
volume: '180'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 8
  - 17
pdf: https://proceedings.mlr.press/v180/zhang22b/zhang22b.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
