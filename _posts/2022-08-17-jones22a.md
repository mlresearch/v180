---
title: 'If you’ve trained one you’ve trained them all: inter-architecture similarity
  increases with robustness'
abstract: Previous work has shown that commonly-used metrics for comparing representations
  between neural networks overestimate similarity due to correlations between data
  points. We show that intra-example feature correlations also causes significant
  overestimation of network similarity and propose an image inversion technique to
  analyze only the features used by a network. With this technique, we find that similarity
  across architectures is significantly lower than commonly understood, but we surprisingly
  find that similarity between models with different architectures increases as the
  adversarial robustness of the models increase. Our findings indicate that robust
  networks tend toward a universal set of representations, regardless of architecture,
  and that the robust training criterion is a strong prior constraint on the functions
  that can be learned by diverse modern architectures. We also find that the representations
  learned by a robust network of any architecture have an asymmetric overlap with
  non-robust networks of many architectures, indicating that the representations used
  by robust neural networks are highly entangled with the representations used by
  non-robust networks.
openreview: BGfLS_8j5eq
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jones22a
month: 0
tex_title: 'If you’ve trained one you’ve trained them all: inter-architecture similarity
  increases with robustness'
firstpage: 928
lastpage: 937
page: 928-937
order: 928
cycles: false
bibtex_author: Jones, Haydn T. and Springer, Jacob M. and Kenyon, Garrett T. and Moore,
  Juston S.
author:
- given: Haydn T.
  family: Jones
- given: Jacob M.
  family: Springer
- given: Garrett T.
  family: Kenyon
- given: Juston S.
  family: Moore
date: 2022-08-17
address:
container-title: Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial
  Intelligence
volume: '180'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 8
  - 17
pdf: https://proceedings.mlr.press/v180/jones22a/jones22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
